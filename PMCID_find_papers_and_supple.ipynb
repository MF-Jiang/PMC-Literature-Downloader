{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: download_pmc_articles\n",
    "\n",
    "Description:\n",
    "    This script simulates searching for articles on PubMed Central (PMC).\n",
    "    For each PMCID in the input file, it checks the PMC webpage to determine:\n",
    "        - Whether a \"PDF\" download button is available.\n",
    "        - Whether there are one or more \"Supplementary\" file links.\n",
    "    If found, the script downloads the PDF and all supplementary files.\n",
    "\n",
    "Outputs:\n",
    "    - missing_row_pmcid_results_temp.csv : temporary log file (updated every 10 articles),\n",
    "      recording whether each article has a downloadable PDF, whether supplementary files exist,\n",
    "      and how many were found.\n",
    "    - missing_row_pmcid_results.csv : final summary file after all downloads are complete.\n",
    "    - failed.txt : records supplementary files that failed to download.\n",
    "\n",
    "Parameters:\n",
    "    csv_path : str\n",
    "        Path to the input CSV file containing the PMCID list.\n",
    "    out_dir : str\n",
    "        Path to the output directory where downloaded files will be saved.\n",
    "        Each article will be stored as:\n",
    "            {PMID}.pdf  → main article\n",
    "            {PMID}_supp_{i}  → supplementary files\n",
    "\n",
    "Notes:\n",
    "    Some failed or error cases may require manual fixing.\n",
    "\"\"\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from wsgiref import headers\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ],
   "id": "bdc4d641e2cfa02c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_col(col: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", col.strip().lower())\n",
    "\n",
    "def find_col(cols, candidates):\n",
    "    norm = {normalize_col(c): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        key = normalize_col(cand)\n",
    "        if key in norm:\n",
    "            return norm[key]\n",
    "    return None\n",
    "\n",
    "def download_file(url, out_path, chunk=1024*256, timeout=10, headless=False, user_agent=None):\n",
    "    import os\n",
    "    import re\n",
    "    import time\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "\n",
    "    import requests\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.common.exceptions import WebDriverException\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "    def _safe_filename_from_cd(cd):\n",
    "        if not cd:\n",
    "            return None\n",
    "        m = re.search(r'filename\\*=UTF-8\\'\\'([^;]+)', cd)\n",
    "        if m:\n",
    "            from urllib.parse import unquote\n",
    "            return unquote(m.group(1))\n",
    "        m = re.search(r'filename=\\\"?([^\\\";]+)\\\"?', cd)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        return None\n",
    "\n",
    "    def _stream_save(resp, path, chunk_size):\n",
    "        tmp = path + \".part\"\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            for chunk_ in resp.iter_content(chunk_size=chunk_size):\n",
    "                if chunk_:\n",
    "                    f.write(chunk_)\n",
    "        os.replace(tmp, path)\n",
    "        return os.path.abspath(path)\n",
    "\n",
    "    def _wait_for_download(tmpdir, before_set, wait_timeout):\n",
    "        end = time.time() + wait_timeout\n",
    "        last_candidate = None\n",
    "        while time.time() < end:\n",
    "            current = set(os.listdir(tmpdir))\n",
    "            new = current - before_set\n",
    "            ready = [f for f in new if not f.endswith((\".crdownload\", \".part\", \".partial\", \".tmp\"))]\n",
    "            if ready:\n",
    "                ready.sort(key=lambda x: os.path.getmtime(os.path.join(tmpdir, x)))\n",
    "                candidate = os.path.join(tmpdir, ready[-1])\n",
    "                s1 = os.path.getsize(candidate)\n",
    "                time.sleep(0.1)\n",
    "                s2 = os.path.getsize(candidate)\n",
    "                if s1 == s2 and s1 > 0:\n",
    "                    return candidate\n",
    "                last_candidate = candidate\n",
    "            time.sleep(0.1)\n",
    "        return last_candidate if last_candidate and os.path.exists(last_candidate) else None\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    if not parsed.scheme:\n",
    "        url = \"http://\" + url\n",
    "        parsed = urlparse(url)\n",
    "\n",
    "    out_path = os.path.abspath(out_path)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    session = requests.Session()\n",
    "    headers = {\"User-Agent\": user_agent or \"Mozilla/5.0\"}\n",
    "\n",
    "    try:\n",
    "        resp = session.get(url, stream=True, timeout=10, allow_redirects=True, headers=headers)\n",
    "        ctype = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "        cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
    "        if (\"application/pdf\" in ctype) or (\"application/octet-stream\" in ctype) or (\"attachment\" in cd.lower()) or url.lower().endswith(\".pdf\"):\n",
    "            fname = _safe_filename_from_cd(cd) or os.path.basename(urlparse(url).path) or f\"file_{int(time.time())}.pdf\"\n",
    "            target = out_path if not os.path.isdir(out_path) else os.path.join(out_path, fname)\n",
    "            return _stream_save(resp, target, chunk)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    tmpdir = tempfile.mkdtemp(prefix=\"selenium_dl_\")\n",
    "    prefs = {\n",
    "        \"download.default_directory\": os.path.abspath(tmpdir),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"plugins.always_open_pdf_externally\": True,\n",
    "    }\n",
    "    opts = Options()\n",
    "\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1400,900\")\n",
    "    opts.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=opts)\n",
    "\n",
    "        origin = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        before = set(os.listdir(tmpdir))\n",
    "\n",
    "        driver.get(origin)\n",
    "        time.sleep(0.5)\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        maybe = _wait_for_download(tmpdir, before, 10)\n",
    "        if maybe:\n",
    "            target = out_path if not os.path.isdir(out_path) else os.path.join(out_path, os.path.basename(maybe))\n",
    "            shutil.move(maybe, target)\n",
    "            return os.path.abspath(target)\n",
    "\n",
    "        xpaths = [\n",
    "            \"//a[contains(@href, '.pdf')]\",\n",
    "            \"//a[@download]\",\n",
    "            \"//a[contains(translate(text(),'PDF','pdf'),'pdf')]\",\n",
    "            \"//button[contains(translate(text(),'DOWNLOAD','download'),'download')]\",\n",
    "        ]\n",
    "        for xp in xpaths:\n",
    "            els = driver.find_elements(\"xpath\", xp)\n",
    "            if els:\n",
    "                for el in els:\n",
    "                    try:\n",
    "                        el.click()\n",
    "                        maybe = _wait_for_download(tmpdir, before, timeout)\n",
    "                        if maybe:\n",
    "                            target = out_path if not os.path.isdir(out_path) else os.path.join(out_path, os.path.basename(maybe))\n",
    "                            shutil.move(maybe, target)\n",
    "                            return os.path.abspath(target)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "        raise RuntimeError(\"no file\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            time.sleep(0.1)\n",
    "            driver.quit()\n",
    "        shutil.rmtree(tmpdir, ignore_errors=True)"
   ],
   "id": "f27e939f23ee3f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "def download_from_pmcid(pmcid, pmid, out_dir, session):\n",
    "\n",
    "    base_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/\"\n",
    "    pdf_url = f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmcid}/pdf\"\n",
    "    supp_dir = Path(out_dir)\n",
    "    main_pdf_path = supp_dir / f\"{pmid}.pdf\"\n",
    "\n",
    "    results = {\"pmcid\": pmcid, \"pmid\": pmid, \"pdf\": False, \"supp\": 0}\n",
    "\n",
    "    r = session.get(base_url, timeout=10)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    # print(soup)\n",
    "    results[\"pdf\"] = False\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if pdf_url in href or re.search(r\"pdf/[^/]+\\.pdf\", href, re.IGNORECASE):\n",
    "                # print(href)\n",
    "                results[\"pdf\"] = True\n",
    "                break\n",
    "    # print(results[\"pdf\"])\n",
    "    try:\n",
    "        if results[\"pdf\"]:\n",
    "            download_file(pdf_url, main_pdf_path, session)\n",
    "            results[\"pdf\"] = True\n",
    "        # print(f\"pdf download success {pmid}.pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"pdf download false {pmcid}: {e}\")\n",
    "\n",
    "    try:\n",
    "        r = session.get(base_url, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"cannot reach ({r.status_code}): {base_url}\")\n",
    "            return results\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        supp_links = [\n",
    "            a[\"href\"]\n",
    "            for a in soup.find_all(\"a\", href=True)\n",
    "            if \"/articles/instance/\" in a[\"href\"] and \"/bin/\" in a[\"href\"]\n",
    "        ]\n",
    "        # supp_links =  np.unique(supp_links)\n",
    "        supp_links = list(dict.fromkeys(supp_links))\n",
    "        if not supp_links:\n",
    "            # print(f\" {pmcid} has no supp\")\n",
    "            return results\n",
    "\n",
    "        # print(f\"{pmcid} found {len(supp_links)} supp links\")\n",
    "\n",
    "        for i, link in enumerate(supp_links, 1):\n",
    "            if link.startswith(\"//\"):\n",
    "                url = \"https:\" + link\n",
    "            elif link.startswith(\"/pmc/\"):\n",
    "                url = \"https://www.ncbi.nlm.nih.gov\" + link\n",
    "            elif link.startswith(\"/articles/instance/\"):\n",
    "                url = \"https://pmc.ncbi.nlm.nih.gov\" + link\n",
    "            elif not link.startswith(\"http\"):\n",
    "                url = \"https://pmc.ncbi.nlm.nih.gov\" + link\n",
    "            else:\n",
    "                url = link\n",
    "\n",
    "            # print(url)\n",
    "            orig_name = os.path.basename(url.split(\"?\")[0])\n",
    "            supp_name = supp_dir / f\"{pmid}_supp_{i}_{orig_name}\"\n",
    "            # ext = os.path.splitext(url.split(\"?\")[0])[1] or \".dat\"\n",
    "            # supp_name = supp_dir / f\"{pmid}_supp_{i}{ext}\"\n",
    "\n",
    "            try:\n",
    "                download_file(url, supp_name, session)\n",
    "                results[\"supp\"] += 1\n",
    "                # print(f\"   supp {i} download success：{supp_name.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   supp {i} download false：{url} ({e})\")\n",
    "                with open(\"failed.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{pmid}:supp{i}:{url}\\n\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"reach supp false {pmcid}: {e}\")\n",
    "\n",
    "    return results\n"
   ],
   "id": "52816257b63411ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "csv_path = \"missing_rows.csv\"\n",
    "out_dir = \"pmc_papers\"\n",
    "sleep = 0.1\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"PMCDownloader/1.0\"\n",
    "})\n",
    "\n",
    "report = []\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "# df = df.head(3)\n",
    "# print(df)\n",
    "i = 0\n",
    "for pmcid, pmid in tqdm(zip(df[\"PMCID\"], df[\"PMID\"]), total=len(df)):\n",
    "    if pd.isna(pmcid):\n",
    "        continue\n",
    "    i = i+1\n",
    "    # if i<2191:\n",
    "    #     continue\n",
    "    result = download_from_pmcid(pmcid, pmid, out_dir, session)\n",
    "    report.append(result)\n",
    "\n",
    "    if i%10 == 0:\n",
    "        temp = pd.DataFrame(report)\n",
    "        temp.to_csv(\"missing_row_pmcid_results_temp.csv\", index=False)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "report = pd.DataFrame(report)\n",
    "report.to_csv(\"missing_row_pmcid_results.csv\", index=False)"
   ],
   "id": "68ab232d32ebd784"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
